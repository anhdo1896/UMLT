{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 4\n",
    "\n",
    "Starter code for workshop 4. You should have seen most of it before, but make sure you understand what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To plot even prettier figures\n",
    "import seaborn as sn\n",
    "\n",
    "# General data handling (pure numerics are better in numpy)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can import the dataset in this way\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray = data.data      # Extract the features\n",
    "yarray = data.target    # Extract the target\n",
    "print(xarray.shape)\n",
    "print(yarray.shape)\n",
    "# We can concatenate features with target\n",
    "fullarray = np.concatenate((xarray,np.reshape(yarray,(-1,1))),axis=1)\n",
    "print(fullarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features names are: {data.feature_names}')\n",
    "print(f'Label names are: {data.target_names}')\n",
    "# We want to consider malignant as the 'positive' class \n",
    "#  - otherwise interpretation gets harder as a positive test in medicine corresponds to more severe disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This point is pretty important\n",
    "# For general convenience, class 1 is the positive class and in general the positive class is the worst condition\n",
    "\n",
    "# Now invert the labels (so that malignant=1)\n",
    "fullarray[:,-1] = 1 - fullarray[:,-1]   \n",
    "df = pd.DataFrame(fullarray,columns = list(data.feature_names) + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the mean of the target column gives the percentage of samples with target=1 (i.e. 37% has target=1, and so 63% have target=0)\n",
    "## If there were more classes it would be important to determine and display the relevant proportions for all classes in another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(df.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = df.hist(bins=40,figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,22))\n",
    "for n in range(fullarray.shape[1]):\n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.plot(np.sort(fullarray[:,n]),'o')\n",
    "    plt.title(df.columns[n])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into separate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bigtrain_set, test_set = train_test_split(fullarray, test_size=0.2, random_state=42)\n",
    "train_set, val_set = train_test_split(bigtrain_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x and y for train, val and test\n",
    "X_train = train_set[:,:-1]\n",
    "y_train = train_set[:,-1]\n",
    "X_test = test_set[:,:-1]\n",
    "y_test = test_set[:,-1]\n",
    "X_val = val_set[:,:-1]\n",
    "y_val = val_set[:,-1]\n",
    "print([X_train.shape,y_train.shape,X_test.shape,y_test.shape,X_val.shape,y_val.shape])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace missing features with median, and scale to std distribution\n",
    "preproc_pl = Pipeline([('imputer', SimpleImputer(strategy=\"median\")), \n",
    "                          ('stdscaler', StandardScaler())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# This is a logistic regression\n",
    "sgd_pl = Pipeline([ ('preproc',preproc_pl), ('sgd',SGDClassifier(loss='log')) ])\n",
    "sgd_pl.fit(X_train,y_train)\n",
    "y_val_pred = sgd_pl.predict(X_val)\n",
    "y_val_prob = sgd_pl.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what is the result in y_val_pred and y_val_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = y_val_prob[:,1]   # just take the positive class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot can help you  to find the errors\n",
    "plt.plot(np.abs(y_val - y_val_pred),'r*')\n",
    "plt.xlabel('Sample Number')\n",
    "plt.ylabel('Error Status')\n",
    "plt.title('Errors - Binary')\n",
    "plt.show()\n",
    "plt.plot(np.abs(y_val - y_val_prob),'b*')\n",
    "plt.xlabel('Sample Number')\n",
    "plt.ylabel('Error Status')\n",
    "plt.title('Errors - Probability Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why in the second plot I have a similar result than the first plot?\n",
    "\n",
    "# Check the values again, but using two decimals only\n",
    "np.round(y_val_prob,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_true=y_val, y_pred=y_val_pred)\n",
    "sn.heatmap(cmat,annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive = worse/most impotant condition = 1\n",
    "# negative = less important condition = 0 \n",
    "\n",
    "# Can you give me?\n",
    "\n",
    "# accuracy = \n",
    "# true_positive (TP) = \n",
    "# true_negative (TN) = \n",
    "# false_positive (FP) = \n",
    "# false_negative (FN) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using normalize = true, you get the proportion of cases related to the rows\n",
    "cmat = confusion_matrix(y_true=y_val, y_pred=y_val_pred, normalize='true')\n",
    "sn.heatmap(cmat,annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalised Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# The most common metric\n",
    "sgd_acc = accuracy_score(y_true = y_val, y_pred = y_val_pred) # -> right predictions / total of samples\n",
    "print(f'Accuracy for SGD is {sgd_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "# Other useful metrics\n",
    "prec = precision_score(y_true = y_val, y_pred = y_val_pred) # -> TP / (TP + FP) \n",
    "recl = recall_score(y_true = y_val, y_pred = y_val_pred)    # -> TP / (TP + FN)\n",
    "print(f'Precision & Recall are {prec} and {recl}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision** gives relevance to the FP cases\n",
    "* **Recall** gives relevance to the FN cases\n",
    "\n",
    "Which one is more important? it depends\n",
    "\n",
    "**Example 1**: You are creating a model to predict the condition of tumors (benign and malignant). What is the most important error?\n",
    "\n",
    "In this context:\n",
    "\n",
    "* False negative (FN) is when you are predicting the tumor is benign, but actually is malignant\n",
    "* False positive (FP) is when you are predicting the tumor is malignant, but actually is benign\n",
    "\n",
    "FN cases could be much worse, so, for this case is more important avoid FN -- > recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: You are creating a pregancy test (using ML) to predict whether a woman is pregnant or not. What is the most important error?\n",
    "\n",
    "In this context:\n",
    "\n",
    "* False negative (FN) is when you are predicting the woman is not pregnant, but actually she is pregnant.\n",
    "* False positive (FP) is when you are predicting the woman is pregnant, but actually is not pregnant.\n",
    "\n",
    "In **my personal opinion**, a FP case is worse, so, you should try to avoid these cases --> precision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Receiver-Operator-Characteristic (ROC)**\n",
    "\n",
    "It illustrates the diagnostic ability of a binary classifier considering different **thresholds**.\n",
    "It compare the performance of:\n",
    "\n",
    "\n",
    "* **TPR** (recall): TP / (TP + FN) --> **How good is my model in predicting correctly the positive class?**\n",
    "* **FPR**: FP / (FP + TN) --> **how often the model makes false positive prediction?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Original curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_prob, pos_label=1)\n",
    "plt.plot(fpr,tpr,'b')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "### EXTENSION CODE ###\n",
    "\n",
    "# Fake a change in one point\n",
    "ridx = np.random.randint(y_val_prob.shape[0])\n",
    "y_val_prob[ridx] = 1 - y_val_prob[ridx]\n",
    "\n",
    "# Curve with the modification\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_prob, pos_label=1)\n",
    "plt.plot(fpr,tpr,'r')\n",
    "plt.title('ROC curve')\n",
    "\n",
    "### END EXTENSION CODE ###\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can see the thresholds used in that model\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can see the samples when your model fails.\n",
    "print(y_val_prob[y_val_pred!=y_val])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *How many distinct points are there?  Try calculating the ROC curve again using the probability outputs instead. Look at the thresholds and compare these to the predicted probability outputs from the classifier just at the points where the binary prediction is wrong.*\n",
    "\n",
    "There are 8 thresholds, representing 2 end points and 6 intermediate points, although several are extremely close to 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC stands for Area Under the Curve\n",
    "auc_sgd = auc(fpr,tpr)\n",
    "print(f'AUC for SGD classifier is {auc_sgd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# This is another illustration to evaluate the performance of a model.\n",
    "prec, recl, thresholds = precision_recall_curve(y_val, y_val_prob, pos_label=1)\n",
    "plt.plot(recl,prec,'b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n",
    "# A higher threshold leads to higher precision, but low recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds)\n",
    "print((np.min(y_val_prob),np.max(y_val_prob)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** *If this classifier would be used to make decisions in the hospital, which threshold would you choose? Is precision more important or recall? Do you think this classifier is good enough or does it need more research?*\n",
    "\n",
    "You should consider this and decide for yourself, but realise that a False Negative is more serious than a False Positive, because if someone has a disease and is left untreated it could be very bad, whereas a healthy person who is told they have a disease will normally undergo more tests and then find out that it was a False Positive. Depending on the disease and the clinical pathway, there may be unnecessary treatment and stress involved for those that are in the False Positive group, but those that are in the False Negative group miss out on earlier treatment, and this can often have fatal consequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "dt_pl = Pipeline([('preproc',preproc_pl), ('dt',DecisionTreeClassifier(random_state=0))])\n",
    "dt_pl.fit(X_train,y_train)\n",
    "y_val_pred_tree = dt_pl.predict(X_val)\n",
    "y_val_prob_tree = dt_pl.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.abs(y_val - y_val_pred_tree),'r*')\n",
    "plt.xlabel('Sample Number')\n",
    "plt.ylabel('Error Status')\n",
    "plt.title('Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the plot of your decision tree\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "dummy = plot_tree(dt_pl['dt'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This plot gives you information about**\n",
    "\n",
    "1) The column or feature used in each split.\n",
    "2) **Gini**: The score given in each split using Gini impurity score\n",
    "3) **samples**: Number of samples used in each split\n",
    "4) **value**: How many samples I have for class 0 and class 1, respectively\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *What do each of the components (nodes, branches, thresholds) of the decision tree mean?*\n",
    "\n",
    "You should be able to answer this on the basis of what was in the lecture. Note that feature numbers in each decision node are expressed as X[n]<=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(y_true=y_val, y_pred=y_val_pred_tree, normalize='true')\n",
    "sn.heatmap(cmat,annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalised Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_acc = accuracy_score(y_true = y_val, y_pred = y_val_pred_tree)\n",
    "print(f'Accuracy for Decision Tree is {dt_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(y_true = y_val, y_pred = y_val_pred_tree)\n",
    "recl = recall_score(y_true = y_val, y_pred = y_val_pred_tree)\n",
    "print(f'Precision & Recall are {prec} and {recl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_prob_tree[:,1], pos_label=1)\n",
    "plt.plot(fpr,tpr,'b')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** *Why are there so few points in the ROC curve?  Does it still show useful information?*\n",
    "\n",
    "You can see that there are only three points (two end points and one intermediate point) but it still indicates the single useful operating point and its associated performance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dt = auc(fpr,tpr)\n",
    "print(f'AUC for Decision Tree classifier is {auc_dt}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** *How does the decision tree compare to the SGD linear model?  List 2 pros and 2 cons of each approach.*\n",
    "\n",
    "AUC is 0.95 for the SGD linear model and 0.92 for the Decision Tree.  The confusion matrix tells a similar story.  Although SGD has better performance, the decision tree is more interpretable.  Can you think of other pros and cons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recl, thresholds = precision_recall_curve(y_val, y_val_prob_tree[:,1], pos_label=1)\n",
    "plt.plot(recl,prec,'b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *What do you think would be a good performance metric to use in this case, and why?  Choose one to work with here.*\n",
    "\n",
    "A good answer here will depend on what you think is most important in the context of the task. If we want to try and supress False Negatives primarily then it would be good to choose an option with a good Recall, but still with acceptable Precision.  Based on the Precision-Recall curves, I would personally choose the SGD, with an operating point nearest to the top right. Looking at the class predictions (y_val_pred) as opposed to the probabilities (y_val_prob) shows that it is already choosing a good operating point, as shown also by the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Comparison based on AUC is {auc_sgd} vs {auc_dt} for SGD vs Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Comparison based on accuracy is {sgd_acc} vs {dt_acc} for SGD vs Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing SGD\n",
    "bestmod = sgd_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmod.fit(np.concatenate((X_train,X_val),axis=0),np.concatenate((y_train,y_val)))\n",
    "y_test_pred = bestmod.predict(X_test)\n",
    "y_test_prob = bestmod.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_prob[:,1], pos_label=1)\n",
    "auc_bm = auc(fpr, tpr)\n",
    "acc_bm = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Final scores on Test set are: AUC = {auc_bm} and Accuracy = {acc_bm}')\n",
    "print(f'For comparison, scores on Validation set are: AUC = {auc_sgd} and Accuracy = {sgd_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** *What would it mean if there was a big difference between the performance scores on the validation and test datasets?*\n",
    "\n",
    "These scores are quite similar and a big difference would not be expected in this case. The validation performance will typically be biased to be higher than it should, based on that fact that it was used to choose the best method, but for a small number of comparisons it would not normally be a large bias. If you do see a large difference then it might either be due to chance, or it may be due to a bug, so it is worth double-checking your code, looking more closely at the results and/or trying another random splitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
